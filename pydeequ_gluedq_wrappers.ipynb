{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQDL Wrappers for PyDeequ\n",
    "\n",
    "This notebook expands upon the [basic_example](basic_example.ipynb) to define a Class having a set of methods similar to the boto3 Glue.Class.   This provides a custom implementation of DLDQ-like functionality with alternate data stores in DynamoDb and S3, while supporting easy future migration to Glue ETL.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Scripts\n",
    "First execute Initialization and Definition cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dq = SimpleDQ()\n",
    "\n",
    "ruleset = {\n",
    " \"Name\": \"cstg_agency_example\",\n",
    " \"ClientToken\": \"string\",\n",
    " \"Description\": \"just a simple sample ruleset\",\n",
    " \"Ruleset\": \"HasSize >= 50, HasMin \\\"penaltyassessmentdays\\\" >= 90, IsComplete \\\"agencyname\\\", IsUnique \\\"agencyname\\\", IsContainedIn \\\"applypenalty\\\" in [\\\"Y\\\",\\\"N\\\"], IsNonNegative \\\"interestrate\\\"\",\n",
    " \"Tags\": {\n",
    "  \"optional\": \"stuff\"\n",
    " },\n",
    " \"TargetTable\": {\n",
    "  \"CatalogId\": \"AwsDataCatalog\",\n",
    "  \"DatabaseName\": \"dtl-prd-smpl0-g2\",\n",
    "  \"TableName\": \"cstg_agency\"\n",
    " }\n",
    "}\n",
    "\n",
    "dq.update_data_quality_ruleset(**ruleset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting cstg_agency_example\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'TargetTable': {'DatabaseName': 'dtl-prd-smpl0-g2',\n",
       "  'CatalogId': 'AwsDataCatalog',\n",
       "  'TableName': 'cstg_agency'},\n",
       " 'Description': 'just a simple sample ruleset',\n",
       " 'Ruleset': 'HasSize >= 50, HasMin \"penaltyassessmentdays\" >= 90, IsComplete \"agencyname\", IsUnique \"agencyname\", IsContainedIn \"applypenalty\" in [\"Y\",\"N\"], IsNonNegative \"interestrate\"',\n",
       " 'ClientToken': 'string',\n",
       " 'Tags': {'optional': 'stuff'},\n",
       " 'Name': 'cstg_agency_example'}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dq = SimpleDQ()\n",
    "\n",
    "ruleset = dq.get_data_quality_ruleset( Name = 'cstg_agency_example' )\n",
    "ruleset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://wc2h-dtl-prd-datalake/PARQUET/cstg_agency\n",
      "Getting cstg_agency_example\n",
      "{\n",
      "  \"Type\": \"HasSize\",\n",
      "  \"ColName\": \"\",\n",
      "  \"Expression\": \">= 50\",\n",
      "  \"Lambda\": \"<function parse_dqdl_rule.<locals>.<lambda> at 0x7f586ebcd480>\",\n",
      "  \"Text\": \"HasSize >= 50\"\n",
      "}\n",
      "{\n",
      "  \"Type\": \"HasMin\",\n",
      "  \"ColName\": \"penaltyassessmentdays\",\n",
      "  \"Expression\": \">= 90\",\n",
      "  \"Lambda\": \"<function parse_dqdl_rule.<locals>.<lambda> at 0x7f5831eb7eb0>\",\n",
      "  \"Text\": \"HasMin \\\"penaltyassessmentdays\\\" >= 90\"\n",
      "}\n",
      "{\n",
      "  \"Type\": \"IsComplete\",\n",
      "  \"ColName\": \"agencyname\",\n",
      "  \"Expression\": \"\",\n",
      "  \"Lambda\": null,\n",
      "  \"Text\": \"IsComplete \\\"agencyname\\\"\"\n",
      "}\n",
      "{\n",
      "  \"Type\": \"IsUnique\",\n",
      "  \"ColName\": \"agencyname\",\n",
      "  \"Expression\": \"\",\n",
      "  \"Lambda\": null,\n",
      "  \"Text\": \"IsUnique \\\"agencyname\\\"\"\n",
      "}\n",
      "{\n",
      "  \"Type\": \"IsContainedIn\",\n",
      "  \"ColName\": \"applypenalty\",\n",
      "  \"Expression\": \"in [\\\"Y\\\",\\\"N\\\"]\",\n",
      "  \"Lambda\": [\n",
      "    \"Y\",\n",
      "    \"N\"\n",
      "  ],\n",
      "  \"Text\": \"IsContainedIn \\\"applypenalty\\\" in [\\\"Y\\\",\\\"N\\\"]\"\n",
      "}\n",
      "{\n",
      "  \"Type\": \"IsNonNegative\",\n",
      "  \"ColName\": \"interestrate\",\n",
      "  \"Expression\": \"\",\n",
      "  \"Lambda\": null,\n",
      "  \"Text\": \"IsNonNegative \\\"interestrate\\\"\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
     ]
    }
   ],
   "source": [
    "dq = SimpleDQ()\n",
    "\n",
    "df_results = dq.start_data_quality_ruleset_evaluation_run( **ruleset)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>check</th>\n",
       "      <th>check_level</th>\n",
       "      <th>check_status</th>\n",
       "      <th>constraint</th>\n",
       "      <th>constraint_status</th>\n",
       "      <th>constraint_message</th>\n",
       "      <th>rule</th>\n",
       "      <th>dqrunid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cstg_agency_example</td>\n",
       "      <td>Error</td>\n",
       "      <td>Error</td>\n",
       "      <td>SizeConstraint(Size(None))</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "      <td>HasSize &gt;= 50</td>\n",
       "      <td>230831-162949040150-interactive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cstg_agency_example</td>\n",
       "      <td>Error</td>\n",
       "      <td>Error</td>\n",
       "      <td>MinimumConstraint(Minimum(penaltyassessmentday...</td>\n",
       "      <td>Failure</td>\n",
       "      <td>Value: 0.0 does not meet the constraint requir...</td>\n",
       "      <td>HasMin \"penaltyassessmentdays\" &gt;= 90</td>\n",
       "      <td>230831-162949040150-interactive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cstg_agency_example</td>\n",
       "      <td>Error</td>\n",
       "      <td>Error</td>\n",
       "      <td>CompletenessConstraint(Completeness(agencyname...</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "      <td>IsComplete \"agencyname\"</td>\n",
       "      <td>230831-162949040150-interactive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cstg_agency_example</td>\n",
       "      <td>Error</td>\n",
       "      <td>Error</td>\n",
       "      <td>UniquenessConstraint(Uniqueness(List(agencynam...</td>\n",
       "      <td>Failure</td>\n",
       "      <td>Value: 0.0 does not meet the constraint requir...</td>\n",
       "      <td>IsUnique \"agencyname\"</td>\n",
       "      <td>230831-162949040150-interactive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cstg_agency_example</td>\n",
       "      <td>Error</td>\n",
       "      <td>Error</td>\n",
       "      <td>ComplianceConstraint(Compliance(applypenalty c...</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "      <td>IsContainedIn \"applypenalty\" in [\"Y\",\"N\"]</td>\n",
       "      <td>230831-162949040150-interactive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cstg_agency_example</td>\n",
       "      <td>Error</td>\n",
       "      <td>Error</td>\n",
       "      <td>ComplianceConstraint(Compliance(interestrate i...</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "      <td>IsNonNegative \"interestrate\"</td>\n",
       "      <td>230831-162949040150-interactive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 check check_level check_status  \\\n",
       "0  cstg_agency_example       Error        Error   \n",
       "1  cstg_agency_example       Error        Error   \n",
       "2  cstg_agency_example       Error        Error   \n",
       "3  cstg_agency_example       Error        Error   \n",
       "4  cstg_agency_example       Error        Error   \n",
       "5  cstg_agency_example       Error        Error   \n",
       "\n",
       "                                          constraint constraint_status  \\\n",
       "0                         SizeConstraint(Size(None))           Success   \n",
       "1  MinimumConstraint(Minimum(penaltyassessmentday...           Failure   \n",
       "2  CompletenessConstraint(Completeness(agencyname...           Success   \n",
       "3  UniquenessConstraint(Uniqueness(List(agencynam...           Failure   \n",
       "4  ComplianceConstraint(Compliance(applypenalty c...           Success   \n",
       "5  ComplianceConstraint(Compliance(interestrate i...           Success   \n",
       "\n",
       "                                  constraint_message  \\\n",
       "0                                                      \n",
       "1  Value: 0.0 does not meet the constraint requir...   \n",
       "2                                                      \n",
       "3  Value: 0.0 does not meet the constraint requir...   \n",
       "4                                                      \n",
       "5                                                      \n",
       "\n",
       "                                        rule                          dqrunid  \n",
       "0                              HasSize >= 50  230831-162949040150-interactive  \n",
       "1       HasMin \"penaltyassessmentdays\" >= 90  230831-162949040150-interactive  \n",
       "2                    IsComplete \"agencyname\"  230831-162949040150-interactive  \n",
       "3                      IsUnique \"agencyname\"  230831-162949040150-interactive  \n",
       "4  IsContainedIn \"applypenalty\" in [\"Y\",\"N\"]  230831-162949040150-interactive  \n",
       "5               IsNonNegative \"interestrate\"  230831-162949040150-interactive  "
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting awswrangler[redshift]\n",
      "  Obtaining dependency information for awswrangler[redshift] from https://files.pythonhosted.org/packages/eb/7f/3f0296b736de88a8b5c918e41053c69948a200939106a4e1fd64c5925c6a/awswrangler-3.3.0-py3-none-any.whl.metadata\n",
      "  Using cached awswrangler-3.3.0-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: boto3<2.0.0,>=1.20.32 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from awswrangler[redshift]) (1.28.28)\n",
      "Requirement already satisfied: botocore<2.0.0,>=1.23.32 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from awswrangler[redshift]) (1.31.28)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from awswrangler[redshift]) (1.22.3)\n",
      "Requirement already satisfied: packaging<24.0,>=21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from awswrangler[redshift]) (21.3)\n",
      "Requirement already satisfied: pandas<3.0.0,>=1.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from awswrangler[redshift]) (2.0.3)\n",
      "Requirement already satisfied: pyarrow>=7.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from awswrangler[redshift]) (12.0.1)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from awswrangler[redshift]) (4.7.1)\n",
      "Collecting redshift-connector<3.0.0,>=2.0.0 (from awswrangler[redshift])\n",
      "  Obtaining dependency information for redshift-connector<3.0.0,>=2.0.0 from https://files.pythonhosted.org/packages/63/86/fb94423bc8c385fdce1bfe2afe720bf415d9df56e137e4f321e13844c498/redshift_connector-2.0.913-py3-none-any.whl.metadata\n",
      "  Downloading redshift_connector-2.0.913-py3-none-any.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m933.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3<2.0.0,>=1.20.32->awswrangler[redshift]) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3<2.0.0,>=1.20.32->awswrangler[redshift]) (0.6.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore<2.0.0,>=1.23.32->awswrangler[redshift]) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore<2.0.0,>=1.23.32->awswrangler[redshift]) (1.26.14)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from packaging<24.0,>=21.1->awswrangler[redshift]) (3.0.9)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas<3.0.0,>=1.2.0->awswrangler[redshift]) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas<3.0.0,>=1.2.0->awswrangler[redshift]) (2023.3)\n",
      "Collecting scramp<1.5.0,>=1.2.0 (from redshift-connector<3.0.0,>=2.0.0->awswrangler[redshift])\n",
      "  Downloading scramp-1.4.4-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from redshift-connector<3.0.0,>=2.0.0->awswrangler[redshift]) (4.12.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from redshift-connector<3.0.0,>=2.0.0->awswrangler[redshift]) (2.31.0)\n",
      "Collecting lxml>=4.6.5 (from redshift-connector<3.0.0,>=2.0.0->awswrangler[redshift])\n",
      "  Obtaining dependency information for lxml>=4.6.5 from https://files.pythonhosted.org/packages/01/ae/ce23856fb6065f254101c1df381050b13adf26088dd554a15776615d470f/lxml-4.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl.metadata\n",
      "  Downloading lxml-4.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from redshift-connector<3.0.0,>=2.0.0->awswrangler[redshift]) (68.0.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from beautifulsoup4<5.0.0,>=4.7.0->redshift-connector<3.0.0,>=2.0.0->awswrangler[redshift]) (2.3.2.post1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<2.0.0,>=1.23.32->awswrangler[redshift]) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3.0.0,>=2.23.0->redshift-connector<3.0.0,>=2.0.0->awswrangler[redshift]) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3.0.0,>=2.23.0->redshift-connector<3.0.0,>=2.0.0->awswrangler[redshift]) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3.0.0,>=2.23.0->redshift-connector<3.0.0,>=2.0.0->awswrangler[redshift]) (2023.5.7)\n",
      "Collecting asn1crypto>=1.5.1 (from scramp<1.5.0,>=1.2.0->redshift-connector<3.0.0,>=2.0.0->awswrangler[redshift])\n",
      "  Downloading asn1crypto-1.5.1-py2.py3-none-any.whl (105 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading redshift_connector-2.0.913-py3-none-any.whl (114 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.7/114.7 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached awswrangler-3.3.0-py3-none-any.whl (394 kB)\n",
      "Downloading lxml-4.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (7.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: asn1crypto, scramp, lxml, redshift-connector, awswrangler\n",
      "Successfully installed asn1crypto-1.5.1 awswrangler-3.3.0 lxml-4.9.3 redshift-connector-2.0.913 scramp-1.4.4\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# cold start\n",
    "pip install pydeequ\n",
    "pip install 'awswrangler[redshift]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# cold start for SageMaker on WC2H -- unzip dependencies from local file, since Maven is blocked\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "S_rootdir = os.getcwd()\n",
    "\n",
    "with zipfile.ZipFile( f\"{S_rootdir}/common/ivy2cache.zip\" ) as z:\n",
    "    z.extractall( f\"{os.environ['HOME']}/.ivy2\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ['AWS_DEFAULT_REGION'] = 'us-gov-west-1'\n",
    "os.environ[\"SPARK_VERSION\"] = '3.0'\n",
    "\n",
    "import awswrangler as wr\n",
    "import pandas as pd\n",
    "pd.DataFrame.iteritems = pd.DataFrame.items # (https://stackoverflow.com/questions/75926636/databricks-issue-while-creating-spark-data-frame-from-pandas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.s3a.endpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ec2-user/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ec2-user/.ivy2/jars\n",
      "com.amazon.deequ#deequ added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-3fbdd046-4fb5-4036-9175-b6f3ddfb10d5;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.amazon.deequ#deequ;1.2.2-spark-3.0 in central\n",
      "\tfound org.scalanlp#breeze_2.12;0.13.2 in central\n",
      "\tfound org.scalanlp#breeze-macros_2.12;0.13.2 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.1 in central\n",
      "\tfound com.github.fommil.netlib#core;1.1.2 in central\n",
      "\tfound net.sf.opencsv#opencsv;2.3 in central\n",
      "\tfound com.github.rwl#jtransforms;2.4.0 in central\n",
      "\tfound junit#junit;4.8.2 in central\n",
      "\tfound org.apache.commons#commons-math3;3.2 in central\n",
      "\tfound org.spire-math#spire_2.12;0.13.0 in central\n",
      "\tfound org.spire-math#spire-macros_2.12;0.13.0 in central\n",
      "\tfound org.typelevel#machinist_2.12;0.6.1 in central\n",
      "\tfound com.chuusai#shapeless_2.12;2.3.2 in central\n",
      "\tfound org.typelevel#macro-compat_2.12;1.1.1 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.5 in central\n",
      ":: resolution report :: resolve 1036ms :: artifacts dl 58ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazon.deequ#deequ;1.2.2-spark-3.0 from central in [default]\n",
      "\tcom.chuusai#shapeless_2.12;2.3.2 from central in [default]\n",
      "\tcom.github.fommil.netlib#core;1.1.2 from central in [default]\n",
      "\tcom.github.rwl#jtransforms;2.4.0 from central in [default]\n",
      "\tjunit#junit;4.8.2 from central in [default]\n",
      "\tnet.sf.opencsv#opencsv;2.3 from central in [default]\n",
      "\torg.apache.commons#commons-math3;3.2 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.1 from central in [default]\n",
      "\torg.scalanlp#breeze-macros_2.12;0.13.2 from central in [default]\n",
      "\torg.scalanlp#breeze_2.12;0.13.2 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.5 from central in [default]\n",
      "\torg.spire-math#spire-macros_2.12;0.13.0 from central in [default]\n",
      "\torg.spire-math#spire_2.12;0.13.0 from central in [default]\n",
      "\torg.typelevel#machinist_2.12;0.6.1 from central in [default]\n",
      "\torg.typelevel#macro-compat_2.12;1.1.1 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.scala-lang#scala-reflect;2.12.0 by [org.scala-lang#scala-reflect;2.12.1] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   16  |   0   |   0   |   1   ||   15  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-3fbdd046-4fb5-4036-9175-b6f3ddfb10d5\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 15 already retrieved (0kB/42ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/30 13:11:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/30 13:11:26 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/08/30 13:11:26 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, Row, DataFrame\n",
    "\n",
    "import sagemaker_pyspark\n",
    "import pydeequ\n",
    "\n",
    "classpath = \":\".join(sagemaker_pyspark.classpath_jars())\n",
    "\n",
    "from pyspark import SparkConf\n",
    "conf = (SparkConf()\n",
    "        .set('fs.s3a.endpoint', 's3-us-gov-west-1.amazonaws.com')\n",
    "       )\n",
    "\n",
    "spark = (SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.driver.extraClassPath\", classpath)\n",
    "    .config(\"spark.jars.packages\", pydeequ.deequ_maven_coord)\n",
    "    .config(\"spark.jars.excludes\", pydeequ.f2j_maven_coord)\n",
    "    .config( conf=conf )\n",
    "    .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function and Class Definitions\n",
    "Will be moved to common module for access from either notebook or ETL jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def parse_dqdl_rule(rule_text): # ToDo move to common module\n",
    "    \"\"\" Transform a DQDL-like rule from string to dict \"\"\"\n",
    "    import json\n",
    "    import re\n",
    "    \n",
    "    s = rule_text.split(' ', 2)\n",
    "\n",
    "    rule = {\n",
    "        'Type' : s[0],\n",
    "        'ColName' : '',\n",
    "        'Expression' : '',\n",
    "        'Lambda' : None,\n",
    "        'Text' : rule_text\n",
    "    }\n",
    "    if '\"' in s[1]:\n",
    "        rule['ColName'] = s[1].replace('\"','')\n",
    "        if len(s) == 3:\n",
    "            rule['Expression'] = s[2]\n",
    "    else:\n",
    "        rule['Expression'] = f\"{s[1]} {s[2]}\"\n",
    "\n",
    "    # transform the Expression into a lambda assertion\n",
    "    if rule['Expression'] == '':\n",
    "        pass\n",
    "    \n",
    "    elif re.search(\"[<=>]\", rule['Expression']):\n",
    "        xpr = rule['Expression'].split()\n",
    "        op =  xpr[0]\n",
    "        val = float(xpr[1])\n",
    "        if op == \"=\":\n",
    "            rule['Lambda'] = lambda x: x == val\n",
    "        elif op == \">\":\n",
    "            rule['Lambda'] = lambda x: x > val\n",
    "        elif op == \"<\":\n",
    "            rule['Lambda'] = lambda x: x < val\n",
    "        elif op == \">=\":\n",
    "            rule['Lambda'] = lambda x: x >= val\n",
    "        elif op == \"<=\":\n",
    "            rule['Lambda'] = lambda x: x <= val\n",
    "        \n",
    "    elif rule['Expression'].startswith('between'):\n",
    "        xpr = rule['Expression'].split()\n",
    "        lo = xpr[1]\n",
    "        hi = xpr[3]\n",
    "        rule['Lambda'] = lambda x: lo < x < hi\n",
    "        \n",
    "    elif rule['Expression'].startswith('in'): \n",
    "        xpr = rule['Expression'].split() # no spaces between list values, please!\n",
    "        inlist = xpr[1][1:-1].replace('\"','').split(',')\n",
    "        #rule['Lambda'] = lambda x: x in inlist\n",
    "        rule['Lambda'] = inlist\n",
    "        \n",
    "    else:\n",
    "        print(\"Can't Parse Expression\")\n",
    "    \n",
    "    print(json.dumps(rule, indent=2, default=str))\n",
    "\n",
    "    return rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def run_pydeequ_checks( df, ruleset_name ): # ToDo move to common module\n",
    "    from pydeequ.checks import Check,CheckLevel\n",
    "    from pydeequ.verification import VerificationSuite,VerificationResult\n",
    "    \n",
    "    dq = SimpleDQ()\n",
    "    ruleset = dq.get_data_quality_ruleset( Name = ruleset_name ) \n",
    "                       \n",
    "    check = Check(spark, CheckLevel.Error, ruleset_name)\n",
    "\n",
    "    rules_list = ruleset['Ruleset'].split(', ')\n",
    "    \n",
    "    for rule_text in rules_list:\n",
    "        rule = parse_dqdl_rule(rule_text)\n",
    "\n",
    "        if rule['Type'] == 'HasSize':\n",
    "            check.hasSize( rule['Lambda'] )\n",
    "        elif rule['Type'] == 'HasMin':\n",
    "            check.hasMin( rule['ColName'], rule['Lambda'] )\n",
    "        elif rule['Type'] == 'IsComplete':\n",
    "            check.isComplete( rule['ColName'] )        \n",
    "        elif rule['Type'] == 'IsUnique':\n",
    "            check.isUnique( rule['ColName'] )        \n",
    "        elif rule['Type'] == 'IsContainedIn':  \n",
    "            check.isContainedIn(rule['ColName'], rule['Lambda'])\n",
    "        elif rule['Type'] == 'IsNonNegative':\n",
    "            check.isNonNegative( rule['ColName'] )\n",
    "        else:\n",
    "            print( f\"Skipping Check {rule['Type']} -- Rule Type  is not implemented.\")\n",
    "        #break\n",
    "\n",
    "    checkResult = VerificationSuite(spark).onData(df).addCheck(check).run()\n",
    "    checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult)\n",
    "    #checkResult_df.show(truncate=False)\n",
    "                                          \n",
    "    return checkResult_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDQ: # ToDo move to common module\n",
    "    ''' Data Quality functionality similar to boto3 Glue.Client '''\n",
    "    import boto3\n",
    "    glue_client = boto3.client('glue')\n",
    "    ddb_resource = boto3.resource('dynamodb')\n",
    "    ruleset_table = ddb_resource.Table('dtl-prd-SMPL0-dqrulesets') # dynamodb table for PyDeeQu rulesets    \n",
    "    ruleset_runs_table = ddb_resource.Table('dtl-prd-SMPL0-dqruleset-eval-runs') # dynamodb table for PyDeeQu rulesets    \n",
    "    mode='PyDeeQu'  \n",
    "\n",
    "    def __init__(self, mode='PyDeeQu', **kwargs) -> None:\n",
    "        if mode == 'GlueDQ':\n",
    "            # pass-thru wrapper for boto3 class Glue.Client *data_quality* methods\n",
    "            print('Glue Data Quality implementation pending availability on Govcloud')\n",
    "        self.mode = mode\n",
    "\n",
    "\n",
    "    def batch_get_data_quality_result(self, **kwargs): pass # ToDo \n",
    "    def cancel_data_quality_rule_recommendation_run(self, **kwargs): pass # ToDo \n",
    "    def cancel_data_quality_ruleset_evaluation_run(self, **kwargs): pass # ToDo\n",
    "\n",
    "    def create_data_quality_ruleset( self, **kwargs):\n",
    "        if type(kwargs['Ruleset']) == list:\n",
    "            \", \".join(kwargs['Ruleset'])\n",
    "\n",
    "        if self.mode == \"GlueDQ\":\n",
    "            response = self.glue_client.create_data_quality_ruleset(\n",
    "                Name = kwargs['Name'],       # str Reqd\n",
    "                Ruleset = kwargs['Ruleset'], # str Reqd \n",
    "                Description = kwargs['Description'],\n",
    "                Tags = kwargs['Tags'],      # dict\n",
    "                TargetTable = kwargs['TargetTable'], # dict\n",
    "                ClientToken = kwargs['ClientTokens']\n",
    "            )\n",
    "            return response   # { 'Name': 'string' }\n",
    "        elif self.mode == \"PyDeeQu\":\n",
    "            self.ruleset_table.put_item(\n",
    "                Item = kwargs\n",
    "            )\n",
    "            return { 'Name' : kwargs['Name'] }\n",
    "\n",
    "\n",
    "    def delete_data_quality_ruleset(self, **kwargs): pass # ToDo\n",
    "\n",
    "    def get_data_quality_result(self, **kwargs): pass # ToDo\n",
    "    \n",
    "    def get_data_quality_rule_recommendation_run(self, **kwargs): pass # ToDo\n",
    "\n",
    "    def get_data_quality_ruleset(self, **kwargs): \n",
    "        print(f\"Getting {kwargs['Name']}\")\n",
    "        if self.mode == \"GlueDQ\":\n",
    "            response = self.glue_client.get_data_quality_ruleset(\n",
    "                Name=kwargs['Name']\n",
    "            )\n",
    "            return response\n",
    "        elif self.mode == \"PyDeeQu\":\n",
    "            response = self.ruleset_table.get_item(\n",
    "                Key = { 'Name' : kwargs['Name'] }\n",
    "            )\n",
    "            return response['Item']\n",
    "       \n",
    "    def get_data_quality_ruleset_evaluation_run(self, **kwargs): pass # ToDo\n",
    "    def list_data_quality_results(self, **kwargs): pass # ToDo\n",
    "    def list_data_quality_rule_recommendation_runs(self, **kwargs): pass # ToDo\n",
    "    def list_data_quality_ruleset_evaluation_runs(self, **kwargs): pass # ToDo\n",
    "    def list_data_quality_rulesets(self, **kwargs): pass # ToDo\n",
    "    def start_data_quality_rule_recommendation_run(self, **kwargs): pass # ToDo\n",
    "    \n",
    "    \n",
    "    def start_data_quality_ruleset_evaluation_run(self, **kwargs): \n",
    "        \n",
    "        if self.mode == \"GlueDQ\":\n",
    "            print('Glue Data Quality implementation pending availability on Govcloud')\n",
    "            \"\"\"\n",
    "            response = self.glue_client.get_data_quality_ruleset( \n",
    "                DataSource = kwargs['DataSource'],    # dict Reqd\n",
    "                Role = kwargs['Role'],                # str Reqd\n",
    "                RulesetNames = kwargs['RulesetNames'],# list [str] Reqd,\n",
    "                NumberOfWorkers = kwargs['NumberOfWorkers'], # int default=5\n",
    "                Timeout = kwargs['Timeout'],          # int default=2880\n",
    "                ClientToken = kwargs['ClientToken'],\n",
    "                AdditionalRunOptions = kwargs['AdditionalRunOptions'],   # dict\n",
    "                AdditionalDataSources = kwargs['AdditionalDataSources']  # dict\n",
    "            )\n",
    "            return response # { 'RunId': 'string' }\n",
    "            \"\"\"\n",
    "        elif self.mode == \"PyDeeQu\":\n",
    "            \n",
    "            \n",
    "            # passed into Glue ETL job\n",
    "            process_parms = {\n",
    "                'DataSource' : {\n",
    "                    'GlueTable' : {\n",
    "                        'DatabaseName' : 'dtl-prd-smpl0-g2',\n",
    "                        'TableName' : 'cstg_agency'\n",
    "                    }\n",
    "                },\n",
    "                'RulesetNames' : [\n",
    "                    'cstg_agency_example',\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            # Three options for DataSource ...\n",
    "            if 'GlueTable' in process_parms['DataSource'].keys(): # GlueDQ\n",
    "                table = process_parms['DataSource']['GlueTable']\n",
    "                s3_url = wr.catalog.get_table_location( database=table['DatabaseName'], table=table['TableName'])\n",
    "\n",
    "            elif 'S3Url' in process_parms['DataSource'].keys():   # Alt #1\n",
    "                s3_url = process_parms['DataSource']['S3Url']\n",
    "                \n",
    "            elif 'Athena' in process_parms['DataSource'].keys():  # Alt #2\n",
    "                s3_url = None\n",
    "                sql = process_parms['Athena']['SQL']\n",
    "                dbname = process_parms['Athena']['DataBase']\n",
    "\n",
    "\n",
    "            # 'lightweight' interactive option for logic that will be implemented in Glue ETL job\n",
    "            if s3_url:\n",
    "                #df_pd = wr.s3.read_parquet(s3_url)\n",
    "                print(s3_url)\n",
    "                df_pd = spark.read.parquet(s3_url.replace( 's3://', 's3a://')+'/*/*')\n",
    "            else:\n",
    "                df_pd = wr.athena.read_sql_query(sql, database=dbname)\n",
    "   \n",
    "            if isinstance(df_pd, pd.DataFrame):\n",
    "                print('Convert Pandas to Spark')\n",
    "                df = spark.createDataFrame(df_pd) \n",
    "            else:\n",
    "                df = df_pd\n",
    " \n",
    "            for ruleset_name in process_parms['RulesetNames']:\n",
    "                df_results = run_pydeequ_checks( df, ruleset_name )  \n",
    "                # ToDo aggregate df_results\n",
    "            \n",
    "            df_pd1 = df_results.toPandas()\n",
    "            for index,rule in enumerate(ruleset_list):\n",
    "                df_pd1.at[index, 'rule'] = rule\n",
    "\n",
    "            import datetime\n",
    "            dqrunid = f'{datetime.datetime.now().strftime(\"%y%m%d-%H%M%S%f\")}-interactive'\n",
    "            df_pd1['dqrunid'] = dqrunid\n",
    "            \n",
    "            return df_pd1\n",
    "    \n",
    "    def update_data_quality_ruleset(self, **kwargs): \n",
    "        if self.mode == \"GlueDQ\":\n",
    "            response = self.glue_client.update_data_quality_ruleset(\n",
    "                Name = kwargs['Name'],\n",
    "                Description = kwargs['Description'],\n",
    "                Ruleset = kwargs['Ruleset']\n",
    "            )\n",
    "            return response\n",
    "        elif self.mode == \"PyDeeQu\":\n",
    "            # 'put_item' works for both create and update\n",
    "            response = self.create_data_quality_ruleset(**kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
